# -*- coding: utf-8 -*-
"""Quora Question Pairs.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1XzHONVcBJlYC-7QKf6cQ3DE4fqbMvg7_

#Import Dependencies and Data

***Mount Google Drive***
"""

from google.colab import drive
drive.mount('/content/drive')

!pip install fuzzywuzzy
import nltk
nltk.download("stopwords")

import pandas as pd
import numpy as np
import spacy
import statistics
from fuzzywuzzy import fuzz
from sklearn.feature_extraction.text import TfidfVectorizer
import nltk
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer
import re
from bs4 import BeautifulSoup

from tqdm.notebook import trange, tqdm
tqdm.pandas(desc="Progress")

# Now you can use `progress_apply` instead of `apply`
# and `progress_map` instead of `map`

import numpy as np
import matplotlib.pyplot as plt

import pickle

import warnings
warnings.filterwarnings("ignore")

#Open Data in Pandas dataframe
path = '/content/drive/My Drive/Case Studies/Quora Question Pairs/train.csv'
#Loading data into pandas dataframe
df = pd.read_csv(path)

df.head()

# info() function to print full summary of the dataframe
df.info()

#drop blank rows 
df.dropna(inplace=True)

"""# Basic Featurization

**Basic Features**

* Lenght of the questions1 & question2
* Occurance frequency of questions1 & question2
* Common minimum words
* Common max words
* Common minimum stopwords
* Common maximum stopwords
* First word equal
* Last word equal
* Absolute length difference
* Mean length
"""

#Occurance frequency of question1
q1freq = df.question1.value_counts()
q1freq=q1freq.T.to_dict()
df['fre_q1'] = df.question1.progress_apply(lambda x: q1freq.get(x))

#Occurance frequency of question2
q2freq = df.question2.value_counts()
q2freq=q2freq.T.to_dict()
df['fre_q2'] = df.question2.progress_apply(lambda x: q2freq.get(x))

#Lenght of the questions1
df['q1_len'] = df.question1.progress_apply(lambda x: len(str(x)))

#Lenght of the questions2
df['q2_len'] = df.question2.progress_apply(lambda x: len(str(x)))

#common word between sentenses
#splitting word into sentences
df['q1_words'] = df.question1.progress_apply(lambda x: set(x.split()))
df['q2_words'] = df.question2.progress_apply(lambda x: set(x.split()))

#finding common words
df['common_words'] = df.progress_apply(lambda x: set(x['q1_words']).intersection(set(x['q2_words'])),axis=1)


#Common minimum stopwords
commom_words_min = []

for (a,b,c) in zip(df.common_words, df.q1_words, df.q2_words): 
    try:
        result = len(a)/min(len(b),len(c))
    except:
        result = np.NaN
        
    commom_words_min.append(result)

df['commom_words_min'] = commom_words_min



#Common max words

commom_words_max = []

for (a,b,c) in zip(df.common_words, df.q1_words, df.q2_words): 
    try:
        result = len(a)/min(len(b),len(c))
    except:
        result = np.NaN
        
    commom_words_max.append(result)

df['commom_words_max'] = commom_words_max

!nltk.download('stopwords')

#finding common stopwords in a question
from nltk.corpus import stopwords
stopwords.words('english')

english_stropwords = set(stopwords.words('english'))

#finding stopword in sentense

def stopwords_sentence(sen):
    stopwords_in_sentense = []
    for w in sen:
        if w in english_stropwords:
                stopwords_in_sentense.append(w)
    return stopwords_in_sentense


#finding stopword in question1 and question2

df['q1_stopwords'] = df.q1_words.progress_apply(lambda x: stopwords_sentence(x))
df['q2_stopwords'] = df.q2_words.progress_apply(lambda x: stopwords_sentence(x))


#common_stropword
df['common_stopwords'] = df.progress_apply(lambda x: set(x['q1_stopwords']).intersection(set(x['q2_stopwords'])),axis=1)


#Common minimum stopwords
commom_stopwords_min = []

for (a,b,c) in zip(df.common_stopwords, df.q1_stopwords, df.q2_stopwords): 
    try:
        result = len(a)/min(len(b),len(c))
    except:
        result = np.NaN
        
    commom_stopwords_min.append(result)

df['commom_stopwords_min'] = commom_stopwords_min



#Common maximum stopwords
commom_stopwords_max = []

for (a,b,c) in tqdm(zip(df.common_stopwords, df.q1_stopwords, df.q2_stopwords)): 
    try:
        result = len(a)/max(len(b),len(c))
    except:
        result = np.NaN
    commom_stopwords_max.append(result)

df['commom_stopwords_max'] = commom_stopwords_max

#first word equal
df['first_equal'] = df.question1.progress_apply(lambda x: x.split(' ', )[0])==df.question2.apply(lambda x: x.split(' ', )[0])
df['first_equal'] = df['first_equal'].replace(True,1)

#Last word equal
df['last_equal'] = df.question1.progress_apply(lambda x: x.split(' ', )[-1])==df.question2.apply(lambda x: x.split(' ', )[-1])
df['last_equal'] = df['last_equal'].replace(True,1)

#Absolute length difference
df['abslote_len_diff'] = df.progress_apply(lambda x: abs(x['q1_len']-x['q2_len']),axis=1)

#Mean length
df['mean_len']  = df.progress_apply(lambda x: statistics.mean([x['q1_len'],x['q2_len']]),axis=1)

"""# Test Pre-processing

**Preprocessing of Text**

* Removing html tags
* Removing Punctuations
* Performing stemming
* Removing Stopwords
* Expanding contractions etc.
"""

# To get the results in 4 decemal points

SAFE_DIV = 0.0001 

#doing processing of text
def preprocess(x):
    
    #characters converting into lower
    x = str(x).lower()
    #replacing values

    #Expanding contraction
    x = x.replace(",000,000", "m").replace(",000", "k").replace("′", "'").replace("’", "'")\
                           .replace("won't", "will not").replace("cannot", "can not").replace("can't", "can not")\
                           .replace("n't", " not").replace("what's", "what is").replace("it's", "it is")\
                           .replace("'ve", " have").replace("i'm", "i am").replace("'re", " are")\
                           .replace("he's", "he is").replace("she's", "she is").replace("'s", " own")\
                           .replace("%", " percent ").replace("₹", " rupee ").replace("$", " dollar ")\
                           .replace("€", " euro ").replace("'ll", " will")
    x = re.sub(r"([0-9]+)000000", r"\1m", x)
    x = re.sub(r"([0-9]+)000", r"\1k", x)
    
    #doing stemming
    porter = PorterStemmer()
    pattern = re.compile('\W')
    #defining type
    if type(x) == type(''):
        x = re.sub(pattern, ' ', x)
    
    
    if type(x) == type(''):
        x = porter.stem(x)
        example1 = BeautifulSoup(x)
        x = example1.get_text()
               
    
    return x

df["clean_question1"] = df["question1"].fillna("").progress_apply(preprocess)
df["clean_question2"] = df["question2"].fillna("").progress_apply(preprocess)

#Remove html tags from a string

def remove_html_tags(text):   
    clean = re.compile('<.*?>')
    return re.sub(clean, '', text)

df["clean_question1"] = df["question1"].fillna("").progress_apply(remove_html_tags)
df["clean_question2"] = df["question2"].fillna("").progress_apply(remove_html_tags)

#Removing Punctuation

def remove_punctuation(string): 
    # punctuation marks 
    punctuations = '''!()-[]{};:'"\,<>./?@#$%^&*_~'''
  
    # traverse the given string and if any punctuation 
    # marks occur replace it with null 
    for x in string.lower(): 
        if x in punctuations: 
            string = string.replace(x, "") 
  
    # Print string without punctuation 
    return string

df["clean_question1"] = df["question1"].fillna("").progress_apply(remove_punctuation)
df["clean_question2"] = df["question2"].fillna("").progress_apply(remove_punctuation)

#Removing Stopwords

nltk.download('stopwords')
#loading stop words
en_stops = stopwords.words("english")

def remove_stopword(string):
  all_words = string.split()
  clean_string = []
  for word in all_words: 
      if word not in en_stops:
          clean_string.append(word)
  return clean_string

df["clean_question1"] = df["question1"].fillna("").progress_apply(remove_stopword)
df["clean_question2"] = df["question2"].fillna("").progress_apply(remove_stopword)

df.head()

#saving clean data
#df.to_csv('/content/drive/My Drive/Case Studies/Quora Question Pairs/clean_train.csv')

"""# Simple Featurization

**Simple Features**

*   Fuzz Ratio
*   Fuzz Partial Ratio
*   Fuzz Token Sort Ratio
*   Token Set Ratio
"""

#fuzz_ratio
#fuzz_ratio : https://github.com/seatgeek/fuzzywuzzy#usage http://chairnerd.seatgeek.com/fuzzywuzzy-fuzzy-string-matching-in-python/

df['fuzz_partial_ratio'] = df.progress_apply(lambda x: fuzz.partial_ratio(str(x['clean_question1']),str(x['clean_question2'])),axis=1)

#fuzz_partial_ratio
#fuzz_partial_ratio : https://github.com/seatgeek/fuzzywuzzy#usage http://chairnerd.seatgeek.com/fuzzywuzzy-fuzzy-string-matching-in-python/


# Register `pandas.progress_apply` and `pandas.Series.map_apply` with `tqdm`
# (can use `tqdm.gui.tqdm`, `tqdm.notebook.tqdm`, optional kwargs, etc.)

#df['fuzz_partial_ratio'] = df.apply(lambda x: fuzz.partial_ratio(str(x['question1']),str(x['question2'])),axis=1)


df['fuzz_ratio'] = df.progress_apply(lambda x: fuzz.ratio(str(x['clean_question1']),str(x['clean_question2'])),axis=1)

#token_sort_ratio
#token_sort_ratio : https://github.com/seatgeek/fuzzywuzzy#usage http://chairnerd.seatgeek.com/fuzzywuzzy-fuzzy-string-matching-in-python/

df['fuzz_token_sort_ratio'] = df.progress_apply(lambda x: fuzz.token_sort_ratio(str(x['clean_question1']),str(x['clean_question2'])),axis=1)

#token_set_ratio
#token_set_ratio : https://github.com/seatgeek/fuzzywuzzy#usage http://chairnerd.seatgeek.com/fuzzywuzzy-fuzzy-string-matching-in-python/

df['token_set_ratio'] = df.progress_apply(lambda x: fuzz.token_set_ratio(str(x['clean_question1']),str(x['clean_question2'])),axis=1)

df.head()

df_basic_simple_features = df.drop(['id','qid1', 'qid2','is_duplicate','q1_words','q2_words','common_words','q1_stopwords','q2_stopwords','common_stopwords'], axis=1)

target =df['is_duplicate']

#Saving basic & simple features
#df_basic_simple_features.to_csv('/content/drive/My Drive/Case Studies/Quora Question Pairs/df_basic_simple_features1.csv')
#target.to_csv('/content/drive/My Drive/Case Studies/Quora Question Pairs/target1.csv')

"""# Splitting Data Into Train and Test"""

#Open Data in Pandas dataframe
#df_basic_simple_features_path = '/content/drive/My Drive/Case Studies/Quora Question Pairs/df_basic_simple_features1.csv'
#Loading data into pandas dataframe
#df_basic_simple_features = pd.read_csv(df_basic_simple_features_path, index_col=0)

#loading target file
#target = pd.read_csv('/content/drive/My Drive/Case Studies/Quora Question Pairs/clean_train.csv', index_col=0)

df_basic_simple_features.head()

"""**Splitting Data into Test and Train**"""

X = df_basic_simple_features

#df.q2_td_idf_vec, df.q1_td_idf_vec,
#y =df['is_duplicate']
y =target['is_duplicate']

from sklearn.model_selection import train_test_split

#Splitting the data into train vs test
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)

print (X_train.shape, y_train.shape, X_test.shape, y_test.shape )

"""# Advance Featurization

**Advance Features**

*   Word Embedding - TF-IDF Weighted Word vector
1. Word Embedding of Training Data
2. Word Embedding of Test Data
"""

#https://medium.com/@ranasinghiitkgp/featurization-of-text-data-bow-tf-idf-avgw2v-tfidf-weighted-w2v-7a6c62e8b097

#TFIDF vectorizer of Train Data

all_questions_train = list(X_train['question1']+X_train['question2'])


vectorizer = TfidfVectorizer(lowercase = False,)
vectorizer.fit_transform(all_questions_train)

# we are converting a dictionary with word as a key, and the idf as a value
train_tf_idf_dictionary = dict(zip(vectorizer.get_feature_names(), list(vectorizer.idf_)))

import spacy.cli
spacy.cli.download("en_core_web_lg")
import en_core_web_lg
nlp = en_core_web_lg.load()

#vector dimention
print(nlp.vocab['What'].vector)

q1_if_idf_train = []

#Word Embedding - TF-IDF Weighted Word vector for Q1
clean_question1 = X_train['question1'].tolist()

for q1 in tqdm(clean_question1):

    #Parse sentese loaded english model
    doc1 = nlp(q1)
    #Number of dimention of vector is 300
    mean_vec1 = np.zeros([len(doc1),300])
    for word1 in doc1:
        #fetching the vector value
        vec1 = word1.vector
        #fetching the idf value
        try:
            idf = train_tf_idf_dictionary[str(word1)]
        except:
            idf = 0
        
        #idf weighted vector
        mean_vec1 += vec1 *idf
    #Compute the final vector for a sentence is average vector all word vector
    mean_vec1 = mean_vec1.mean(axis = 0)
    q1_if_idf_train.append(mean_vec1)

q1_if_idf_train = pd.DataFrame(q1_if_idf_train)

dx=[]
for i in range(1,301):
  dx.append("x{}".format(i))
  i+=1

q1_if_idf_train.columns = dx
q1_if_idf_train.head()

#q1_if_idf_train.to_csv('/content/drive/My Drive/Case Studies/Quora Question Pairs/q1_if_idf_train_1.csv')

q2_if_idf_train = []

#Word Embedding - TF-IDF Weighted Word vector for Q1
question2 = X_train['question2'].tolist()

for q2 in tqdm(question2):

    #Parse sentese loaded english model
    doc2 = nlp(q2)
    #Number of dimention of vector is 300
    mean_vec2 = np.zeros([len(doc2),300])
    for word2 in doc2:
        #fetching the vector value
        vec2 = word2.vector
        #fetching the idf value
        try:
            idf = train_tf_idf_dictionary[str(word2)]
        except:
            idf = 0
        
        #idf weighted vector
        mean_vec2 += vec2 *idf
    #Compute the final vector for a sentence is average vector all word vector
    mean_vec2 = mean_vec2.mean(axis = 0)
    q2_if_idf_train.append(mean_vec2)

q2_if_idf_train = pd.DataFrame(q2_if_idf_train)

dy=[]
for i in range(1,301):
  dy.append("y{}".format(i))
  i+=1

q2_if_idf_train.columns = dy
q2_if_idf_train.head()

#q2_if_idf_train.to_csv('/content/drive/My Drive/Case Studies/Quora Question Pairs/q2_if_idf_train_1.csv')

X_train.head()

simplefeature_training = X_train.drop(['question1','question2','clean_question1','clean_question2'], axis=1)

simplefeature_training.head()

simplefeature_training.reset_index(inplace=True)

simplefeature_training = simplefeature_training.drop(['index'],axis=1)

simplefeature_training.head()

print (simplefeature_training.shape, q1_if_idf_train.shape, q2_if_idf_train.shape, y_train.shape)

x_train_concat = pd.concat([simplefeature_training, q1_if_idf_train,q2_if_idf_train], axis=1)

x_train_concat.shape

x_train_concat.head()

#x_train_concat.to_csv('/content/drive/My Drive/Case Studies/Quora Question Pairs/x_train_concat.csv')

#x_train_concat = pd.read_csv('/content/drive/My Drive/Case Studies/Quora Question Pairs/x_train_concat.csv', index_col=0)

y_train

x_train_concat.head()

print(x_train_concat.shape, y_train.shape,X_test.shape,y_test.shape )

"""# Preparing Test Data - Advance Features"""

X_test.head()

#https://medium.com/@ranasinghiitkgp/featurization-of-text-data-bow-tf-idf-avgw2v-tfidf-weighted-w2v-7a6c62e8b097

#TFIDF vectorizer of Train Data

all_questions_test = list(X_test['question1']+X_test['question2'])


vectorizer = TfidfVectorizer(lowercase = False,)
vectorizer.fit_transform(all_questions_test)

# we are converting a dictionary with word as a key, and the idf as a value
test_tf_idf_dictionary = dict(zip(vectorizer.get_feature_names(), list(vectorizer.idf_)))

import spacy.cli
spacy.cli.download("en_core_web_lg")
import en_core_web_lg
nlp = en_core_web_lg.load()

#vector dimention
print(nlp.vocab['quora'].vector)

q1_if_idf_test = []

#Word Embedding - TF-IDF Weighted Word vector for Q1
clean_question1 = X_test['question1'].tolist()

for q1 in tqdm(clean_question1):

    #Parse sentese loaded english model
    doc1 = nlp(q1)
    #Number of dimention of vector is 300
    mean_vec1 = np.zeros([len(doc1),300])
    for word1 in doc1:
        #fetching the vector value
        vec1 = word1.vector
        #fetching the idf value
        try:
            idf = test_tf_idf_dictionary[str(word1)]
        except:
            idf = 0
        
        #idf weighted vector
        mean_vec1 += vec1 *idf
    #Compute the final vector for a sentence is average vector all word vector
    mean_vec1 = mean_vec1.mean(axis = 0)
    q1_if_idf_test.append(mean_vec1)

q1_if_idf_test = pd.DataFrame(q1_if_idf_test)

dx=[]
for i in range(1,301):
  dx.append("x{}".format(i))
  i+=1

q1_if_idf_test.columns = dx
q1_if_idf_test.head()

#q1_if_idf_test.to_csv('/content/drive/My Drive/Case Studies/Quora Question Pairs/q1_if_idf_test1.csv')

q2_if_idf_test = []

#Word Embedding - TF-IDF Weighted Word vector for Q2
clean_question2 = X_test['question2'].tolist()

for q2 in tqdm(clean_question2):

    #Parse sentese loaded english model
    doc2 = nlp(q2)
    #Number of dimention of vector is 300
    mean_vec2 = np.zeros([len(doc2),300])
    for word2 in doc2:
        #fetching the vector value
        vec2 = word2.vector
        #fetching the idf value
        try:
            idf = test_tf_idf_dictionary[str(word2)]
        except:
            idf = 0
        
        #idf weighted vector
        mean_vec2 += vec2 *idf
    #Compute the final vector for a sentence is average vector all word vector
    mean_vec2 = mean_vec2.mean(axis = 0)
    q2_if_idf_test.append(mean_vec2)

q2_if_idf_test = pd.DataFrame(q2_if_idf_test)

dy=[]
for i in range(1,301):
  dy.append("y{}".format(i))
  i+=1

q2_if_idf_test.columns = dy
q2_if_idf_test.head()

#q2_if_idf_test.to_csv('/content/drive/My Drive/Case Studies/Quora Question Pairs/q2_if_idf_test.csv')

X_test.head()

simplefeature_test = X_test.drop(['question1','question2','clean_question1','clean_question2'], axis=1)

simplefeature_test.head()

simplefeature_test.reset_index(inplace=True)

simplefeature_test = simplefeature_test.drop(['index'],axis=1)

simplefeature_test.head()

print(simplefeature_test.shape,q1_if_idf_test.shape, q2_if_idf_test.shape )

x_test_concat = pd.concat([simplefeature_test, q1_if_idf_test,q2_if_idf_test], axis=1)

print(x_test_concat.shape)

#x_test_concat.to_csv('/content/drive/My Drive/Case Studies/Quora Question Pairs/x_test_concat.csv')

#y_train.to_csv('/content/drive/My Drive/Case Studies/Quora Question Pairs/y_train.csv')
#y_test.to_csv('/content/drive/My Drive/Case Studies/Quora Question Pairs/y_test.csv')

#Loading Data
'''
x_train_concat =  pd.read_csv('/content/drive/My Drive/Case Studies/Quora Question Pairs/x_train_concat.csv', index_col=0)
x_test_concat = pd.read_csv('/content/drive/My Drive/Case Studies/Quora Question Pairs/x_test_concat.csv', index_col=0)
y_train = pd.read_csv('/content/drive/My Drive/Case Studies/Quora Question Pairs/y_train.csv', index_col=0)
y_test = pd.read_csv('/content/drive/My Drive/Case Studies/Quora Question Pairs/y_test.csv', index_col=0)
'''

print(x_train_concat.shape,y_train.shape, x_test_concat.shape,y_test.shape )

"""# XGBoost Model Training

## Hyper Parameter Tunning Using Ramdom Search

**Hyper Parameter XGBoost**

* max_depth : int
  * Maximum tree depth for base learners.
* learning_rate : float
    * Boosting learning rate (xgb's "eta")
* n_estimators : int
    * Number of boosted trees to fit.
* silent : boolean
    * Whether to print messages while running boosting.
* objective : string
    * Specify the learning task and the corresponding learning objective.
* nthread : int
    * Number of parallel threads used to run xgboost.
* gamma : float
    * Minimum loss reduction required to make a further partition 
    on a leaf node of the tree.
* min_child_weight : int
    * Minimum sum of instance weight(hessian) needed in a child.
* max_delta_step : int
    * Maximum delta step we allow each tree's weight estimation to be.
* subsample : float
    * Subsample ratio of the training instance.
* colsample_bytree : float
    * Subsample ratio of columns when constructing each tree.
* base_score:
    * The initial prediction score of all instances, global bias.
* seed : int
    * Random number seed.
* missing : float, optional
    * Value in the data which needs to be present as a missing value. If None, defaults to np.nan.
"""

import xgboost as xgb
from sklearn.metrics.classification import accuracy_score,log_loss
from sklearn.metrics import log_loss
from sklearn.model_selection import RandomizedSearchCV

#https://xgboost.readthedocs.io/en/latest/parameter.html
#https://towardsdatascience.com/from-zero-to-hero-in-xgboost-tuning-e48b59bfaf58
#https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html
#https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/


# Create XGB Classifier object
xgb_clf =xgb.XGBClassifier(base_score=0.5, booster='gbtree', objective='binary:logistic', eval_metric='logloss', )
#xgb_clf =xgb.XGBClassifier(base_score=0.5, booster='gpu_hist', objective='binary:logistic', eval_metric='logloss',)

# Create parameter grid
params = {}


params['learning_rate'] = [0.05,0.1,0.2]
params['max_depth'] = [4,5,6]
params['reg_lambda'] = [1,2,3,4,5]
params['reg_alpha'] = [0, 0.005, 0.05, 0.01 , 0.1 ]
params['gamma'] = [00.1, 0.3, 0.5, 1]
#params['colsample_bytree'] = [0.2, 0.4, 0.6, 0.8, 1.0]
#params['subsample'] = [0.2, 0.4, 0.6, 0.8, 1.0]
params['n_estimator'] = [500,1000,2000]

# Create RandomizedSearchCV Object
random_cfl=RandomizedSearchCV(xgb_clf, param_distributions=params, n_jobs=1, cv=2, verbose = 3, n_iter=10, scoring = 'accuracy', random_state = 42,)

# Fit the model
random_cfl.fit(x_train_concat,y_train)

print(random_cfl.best_score_)
print(random_cfl.best_estimator_)

random_cfl.best_params_

"""## Traninig Model on Best Hyperparameter"""

import xgboost as xgb
#from sklearn.metrics.classification import accuracy_score,log_loss
from sklearn.metrics import log_loss
from sklearn.model_selection import RandomizedSearchCV

#https://xgboost.readthedocs.io/en/latest/parameter.html
# Hyperparameter grid
params = {}
params['objective'] = 'binary:logistic'
params['booster'] = 'gbtree'
params['eval_metric'] = 'logloss'
params['learning_rate'] = 0.2
params['gamma'] = 0.3
params['max_depth'] = 6
params['reg_alpha'] = 0.01
params['reg_lambda'] = 5

#n_estimator and num_boost_round are same, n_estimator is Scikit Learn Attribute where num_boost_round is XGBoost attribute
num_round  = 500

d_train = xgb.DMatrix(x_train_concat, label = y_train)
d_test = xgb.DMatrix(x_test_concat, label = y_test)

watchlist  = [(d_train, 'train'),(d_test,'valid')]


#model training
model  = xgb.train(params , d_train,  num_round, watchlist, early_stopping_rounds = 20)

"""## Predicting and Testing Accuracy"""

from sklearn.metrics import accuracy_score

#predict
predict_y = model.predict(d_test)

predictions = [round(value) for value in predict_y]
# evaluate predictions
accuracy = accuracy_score(y_test, predictions)
logloss_score = log_loss(y_test, predict_y)

print("Accuracy: {}%" .format(accuracy * 100.0))
print("Logloss: {}%" .format(logloss_score))

#Ref: https://machinelearningmastery.com/avoid-overfitting-by-early-stopping-with-xgboost-in-python/

from sklearn.metrics import confusion_matrix
import seaborn as sns

def plot_confusion_matrix(y_test, predict_y):
    C = confusion_matrix(y_test, predict_y)
    # C = 9,9 matrix, each cell (i,j) represents number of points of class i are predicted class j
    
    A =(((C.T)/(C.sum(axis=1))).T)
    #divid each element of the confusion matrix with the sum of elements in that column
    
    # C = [[1, 2],
    #     [3, 4]]
    # C.T = [[1, 3],
    #        [2, 4]]
    # C.sum(axis = 1)  axis=0 corresonds to columns and axis=1 corresponds to rows in two diamensional array
    # C.sum(axix =1) = [[3, 7]]
    # ((C.T)/(C.sum(axis=1))) = [[1/3, 3/7]
    #                           [2/3, 4/7]]

    # ((C.T)/(C.sum(axis=1))).T = [[1/3, 2/3]
    #                           [3/7, 4/7]]
    # sum of row elements = 1
    
    B =(C/C.sum(axis=0))
    #divid each element of the confusion matrix with the sum of elements in that row
    # C = [[1, 2],
    #     [3, 4]]
    # C.sum(axis = 0)  axis=0 corresonds to columns and axis=1 corresponds to rows in two diamensional array
    # C.sum(axix =0) = [[4, 6]]
    # (C/C.sum(axis=0)) = [[1/4, 2/6],
    #                      [3/4, 4/6]] 
    plt.figure(figsize=(20,4))
    
    labels = [1,2]
    # representing A in heatmap format
    cmap=sns.light_palette("blue",as_cmap=True)
    plt.subplot(1, 3, 1)
    sns.heatmap(C, annot=True, cmap=cmap, fmt=".3f", xticklabels=labels, yticklabels=labels)
    plt.xlabel('Predicted Class')
    plt.ylabel('Original Class')
    plt.title("Confusion matrix")
    
    plt.subplot(1, 3, 2)
    sns.heatmap(B, annot=True, cmap=cmap, fmt=".3f", xticklabels=labels, yticklabels=labels)
    plt.xlabel('Predicted Class')
    plt.ylabel('Original Class')
    plt.title("Precision matrix")
    
    plt.subplot(1, 3, 3)
    # representing B in heatmap format
    sns.heatmap(A, annot=True, cmap=cmap, fmt=".3f", xticklabels=labels, yticklabels=labels)
    plt.xlabel('Predicted Class')
    plt.ylabel('Original Class')
    plt.title("Recall matrix")
    
    plt.show()

plot_confusion_matrix(y_test, predict_y.round())

"""# Saving Model Using Pickel"""

# save the model to disk
import pickle
filepath = '/content/drive/My Drive/Case Studies/Quora Question Pairs/finalized_model.sav'
pickle.dump(model, open(filepath, 'wb'))

"""# Loading Model from using Pickel"""

# load the model from disk
#loaded_model = pickle.load(open(filepath, 'rb'))
result = loaded_model.score(X_test, Y_test)
#print(result)